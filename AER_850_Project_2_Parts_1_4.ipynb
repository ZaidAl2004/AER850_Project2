{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxrLpZLJ8ORU0QWaVEsrOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZaidAl2004/AER850_Project2/blob/main/AER_850_Project_2_Parts_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkZ9kXWcne_Y"
      },
      "outputs": [],
      "source": [
        "# AER 850: Project 2 Parts 1-4\n",
        "# By: Zaid Al-lababidi\n",
        "# 501176747\n",
        "\n",
        "\n",
        "# Initializing all relevent libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#Issue: can't import image data generator without including tensorflow. even\n",
        "#even though keras was imported earlier\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#Did not end up utilizing this\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "#Step 1: Data Processing\n",
        "# Define all image dimemsions\n",
        "image_height = 500\n",
        "image_width = 500\n",
        "channels = 3\n",
        "#I need to ask as to why channels = 3 instead of 1 since it is gray-scaled.\n",
        "\n",
        "\n",
        "#Define path to training, validation and testing data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/project_2_data/Data/train'\n",
        "val_dir   = '/content/drive/MyDrive/project_2_data/Data/valid'\n",
        "test_dir  = '/content/drive/MyDrive/project_2_data/Data/test'\n",
        "\n",
        "\n",
        "#Define an image data generator that is able to change and rescale data in a certain way\n",
        "#The point of this function is to introduce variance into\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255, #Will make all pixel values in the range of 0-1\n",
        "    shear_range = 0.2, #Will randomly slant the image by up to +-20%\n",
        "    zoom_range = 0.2, #Will randomly zoom in to image by up to +-20%\n",
        "    )\n",
        "\n",
        "\n",
        "#Defining image data generator\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "#Creating Data Generator for training and validation data\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir, #calls the path to obtain data from\n",
        "    target_size = (image_height, image_width), #defines size of each image\n",
        "    batch_size = 32, #determines the batch size\n",
        "    class_mode = \"categorical\", #makes sure that data is pretty much labelled\n",
        "    color_mode='rgb', #This makes sure that the channels are 3 (which is weird since it's in grayscale)\n",
        "    )\n",
        "\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size = (image_height, image_width),\n",
        "    batch_size = 32,\n",
        "    class_mode = \"categorical\",\n",
        "    color_mode='rgb',\n",
        "    )\n",
        "\n",
        "\n",
        "#Checking if data was saved properly\n",
        "print(train_gen.class_indices)\n",
        "\n",
        "\n",
        "images, labels = next(train_gen)\n",
        "print(\"Image Batch Shape\", images.shape)\n",
        "print(\"Labels batch shape:\", labels.shape)\n",
        "\n",
        "\n",
        "#Plotting he first 5 images:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(5):\n",
        "    ax = plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(images[i])\n",
        "    label_index = np.argmax(labels[i])\n",
        "    class_name = list(train_gen.class_indices.keys())[label_index]\n",
        "    plt.title(class_name)\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Verifying data has been normalized properly\n",
        "print(\"Min pixel value:\", np.min(images))\n",
        "print(\"Max pixel value:\", np.max(images))\n",
        "print(\"Mean pixel value:\", np.mean(images))\n",
        "\n",
        "\n",
        "#Step 2: Neural Network Architecture Design\n",
        "#Defining Input Shape\n",
        "input_shape = (500, 500, 3)\n",
        "#Initializing the model\n",
        "#Ask TA if model optimization happens here or later when compiling?\n",
        "#The following line was put in to solve an environment problem\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "mdl = Sequential([\n",
        "    #First Convulation Block -> Captures Simpler Features\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    #Second Convulation Block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    #Third Convulation Block -> Capture More Complex Features\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Number of Filters: reasoning?\n",
        "    # Kernel Size: reasoning?\n",
        "\n",
        "    #Flatten the features to be inputted into neurons\n",
        "    layers.Flatten(),\n",
        "\n",
        "    #First Dense layer\n",
        "    layers.Dense(256, activation='relu'), #Dense layer to learn all features\n",
        "    layers.Dropout(0.5), #Regulize layers, reducing overfitting\n",
        "\n",
        "    #Second Dense Layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    #Output Layer\n",
        "    layers.Dense(3, activation='softmax') #3 neurons for each class type\n",
        "    #Softmax is being used since its multi-class classification\n",
        "    ])\n",
        "\n",
        "\n",
        "#Step 3: Hyperparameter Tuning\n",
        "mdl.compile(\n",
        "    optimizer = 'adam', #Starting with adam\n",
        "    loss = 'categorical_crossentropy', #Starting with categorical cross entropy\n",
        "    metrics = ['accuracy'] #Metric should be changed based on what we need\n",
        "    )\n",
        "\n",
        "#Repeat previous steps for new model\n",
        "\"\"\"\n",
        "mdl2 = Sequential([\n",
        "    #First Convulation Block -> Captures Simpler Features\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "\n",
        "    #Flatten the features to be inputted into neurons\n",
        "    layers.Flatten(),\n",
        "\n",
        "    #First Dense layer\n",
        "    layers.Dense(128, activation='relu'), #Dense layer to learn all features\n",
        "    layers.Dropout(0.5), #Regulize layers, reducing overfitting\n",
        "\n",
        "    #Output Layer\n",
        "    layers.Dense(3, activation='softmax') #3 neurons for each class type\n",
        "    #Softmax is being used since its multi-class classification\n",
        "    ])\n",
        "\n",
        "\n",
        "mdl2.compile(\n",
        "    optimizer = 'adam', #Starting with adam\n",
        "    loss = 'categorical_crossentropy', #Starting with categorical cross entropy\n",
        "    metrics = ['accuracy'] #Metric should be changed based on what we need\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "#Early stopping defined here, ensures model will stop once loss of accuracy is detected\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_accuracy', #Monitor accuracy\n",
        "    patience = 5, #number of epochs with no improvement before stopping\n",
        "    restore_best_weights=True, #Will go back to previous run with better accuracy\n",
        "    verbose=1 #Will print out a message mentioning early stopping\n",
        "    )\n",
        "\n",
        "\n",
        "#Defining batch size and epochs\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "#Fitting the model to training data using validation data to evaluate\n",
        "history = mdl.fit(\n",
        "    train_gen,\n",
        "    validation_data = val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks = [early_stop],\n",
        "    verbose=1\n",
        "    )\n",
        "\n",
        "#Creating the Accuracy plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title(\"Model 1 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Creating the loss plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Model 1 Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Repeating for 2nd Model\n",
        "\"\"\"\n",
        "history2 = mdl2.fit(\n",
        "    train_gen,\n",
        "    validation_data = val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks = [early_stop],\n",
        "    verbose=1\n",
        "    )\n",
        "\n",
        "#Creating the Accuracy plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history2.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title(\"Model 2 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Creating the loss plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history2.history['loss'], label='Training Loss')\n",
        "plt.plot(history2.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Model 2 Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\n",
        "#Saving the model to an external file on google drive\n",
        "save_path = '/content/drive/MyDrive/model1.keras'\n",
        "mdl.save(save_path)\n",
        "print(\"Model was successfully saved to: \", save_path)\n",
        "\n",
        "\"\"\"\n",
        "save_path2 = '/content/drive/MyDrive/model2.keras'\n",
        "mdl2.save(save_path2)\n",
        "print(\"Model 2 was successfully saved to: \", save_path2)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}